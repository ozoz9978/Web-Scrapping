{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fc1ea2-4343-4bd3-9456-85248ca80d36",
   "metadata": {},
   "source": [
    "# request 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a8328-6299-4a4d-b611-861edec17783",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a393c0-2995-4f4d-b8db-0e34c23088b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'https://www.naver.com'\n",
    "response = requests.get(URL) # get 방식으로 요청\n",
    "print(response.status_code)\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "URL = 'https://search.naver.com/search.naver'\n",
    "query = {'query':'python'}\n",
    "response = requests.get(URL, params=query)\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b81e3b-1386-44fb-a0d7-8b307f18230b",
   "metadata": {},
   "source": [
    "# user_agent 값 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417f65b-6d4c-4256-b38f-51162c6c4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "- 로봇이 아님을 나타내기 위해서  user-agent라는 값을 header에 넣어서 보냄\n",
    "- 직접적인 URL주소로 요청 시 웹 사이트에서 웹 크롤링을 통해 접근한 것으로 감지하고 접속을 차단하게 됨\n",
    "- user_agent 헤더값을 포함하여 요청하면 브라우저를 통해 요청하는 것으로 인식되어 해결\n",
    "- 웹 브라우저 실행 -> F12 개발자 모드 진입 -> console에 navigator.userAgent 입력\n",
    "\n",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c3845-288c-4519-ade1-6594af28e437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "URL = 'http://www.google.com/search'\n",
    "params = {'q':'python'}\n",
    "headers = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'}\n",
    "\n",
    "response = requests.get(URL, params = params)\n",
    "response.raise_for_status() #응답코드가 200이 아니면 오류내고 멈춤\n",
    "result = response.text\n",
    "with open('mygoogle.html','w', encoding='utf-8') as f:\n",
    "    f.write(result)\n",
    "print('저장완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc7a1a-e492-47ab-b126-03ceaa1c272a",
   "metadata": {},
   "source": [
    "## [실습] 네이버 데이터랩에서 실시간 인기 검색어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ecdd4-e284-40b2-96c9-ca9070362c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "response = requests.get('https://datalab.naver.com')\n",
    "html_text = response.text\n",
    "\n",
    "temp = html_text.split('<em class=\"num\">1</em>')[1] #()안의 값 밑 전체의 내용이 나오게 됨. \n",
    "temp = temp.split('<span class=\"title\">')[1]\n",
    "temp = temp.split('</span>')[0]\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975913b3-b319-4ca6-9c3a-2679b4335fd0",
   "metadata": {},
   "source": [
    "# BeautifulSoup 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fe488-042f-4f17-a934-c9a26bc49e3f",
   "metadata": {},
   "source": [
    "## Parser별 출력 결과 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af54e92-9640-4d09-bb81-73da74c1bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install html5lib\n",
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e27437-2f07-487a-9c1d-f30cf956277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup ('<a></p>', 'html.parser')\n",
    "print('html.parser')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup = BeautifulSoup ('<a> </p>', 'lxml')\n",
    "print('lxml')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup = BeautifulSoup ('<a> </p>', 'xml')\n",
    "print('xml')\n",
    "print(soup)\n",
    "print('-'*40)\n",
    "\n",
    "soup = BeautifulSoup ('<a> </p>', 'html5lib')\n",
    "print('html5lib')\n",
    "print(soup)\n",
    "print('-'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758444c6-0ce9-4cf0-a3b6-959c670ddb4d",
   "metadata": {},
   "source": [
    "## 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188161ad-8907-4e7f-b32d-b22f320e4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://ko.wikipedia.org/wiki/%EC%9B%B9_%ED%81%AC%EB%A1%A4%EB%9F%AC'\n",
    "response = requests.get(URL)\n",
    "\n",
    "# soup 객체 생성\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# 태그를 이용한 접근\n",
    "print(soup.title)\n",
    "print(soup.footer.ul.li.text)\n",
    "\n",
    "# 태그와 속성을 이용한 접근\n",
    "print(soup.a) #soup 객체에서 첫 번째로 만나는 a 태그 출력\n",
    "#print(soup.a['id']) #만약 속성이 존재하지 않으면 에러 발생\n",
    "print(soup.a['href'])\n",
    "\n",
    "#find() 함수를 이용한 태그 내의 다양한 속성을 이용한 접근\n",
    "print(soup.find('a', attrs={'title':'구글봇'})) # a 태그 중 title 속성의 값이 '구글봇'인 데이터 검색"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783d1e1-ef46-4bdd-9954-c2541365700d",
   "metadata": {},
   "source": [
    "##자식 태그들을 반복 가능한 객체로 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de79e8-3433-43da-ab55-1fe9d33ac564",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title> Web Scrapping </title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align = \"center\">text1</p>\n",
    "        <p class=\"b\" align = \"center\">text2</p>\n",
    "        <p class=\"c\" align = \"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"\\source\" width=\"300\" height=\"200\">\n",
    "        </div>   \n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "#print(contents)\n",
    "for child in contents.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a970d28-9573-4e2b-877c-7f164325a426",
   "metadata": {},
   "source": [
    "## 자신을 포함한 부모 태그까지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a6b15-cd75-4ad0-9aae-4dcb4a4d9b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title> Web Scrapping </title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align = \"center\">text1</p>\n",
    "        <p class=\"b\" align = \"center\">text2</p>\n",
    "        <p class=\"c\" align = \"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"\\source\" width=\"300\" height=\"200\">\n",
    "        </div>   \n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag)\n",
    "print('/n')\n",
    "print(img_tag.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579f96ef-1d8d-4bbd-abc6-e18115798f1e",
   "metadata": {},
   "source": [
    "## 특정 부모 태그까지 검색해서 올라가는 법 (바로 위말고 그이상까지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04990ad5-7c71-4e99-9aac-387b3490a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title> Web Scrapping </title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align = \"center\">text1</p>\n",
    "        <p class=\"b\" align = \"center\">text2</p>\n",
    "        <p class=\"c\" align = \"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"\\source\" width=\"300\" height=\"200\">\n",
    "        </div>   \n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "contents = soup.find('body')\n",
    "img_tag = contents.find('img')\n",
    "print(img_tag.find_parent('body')) #  지정된 부모 태그까지 올라감"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ac7f2-5794-4d6c-9493-25dd555c1cec",
   "metadata": {},
   "source": [
    "## 형제 태그 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc30aa-324e-4489-a9a6-505c35d568ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "    <head>\n",
    "        <title> Web Scrapping </title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <p class=\"a\" align = \"center\">text1</p>\n",
    "        <p class=\"b\" align = \"center\">text2</p>\n",
    "        <p class=\"c\" align = \"center\">text3</p>\n",
    "        <div>\n",
    "            <img src=\"\\source\" width=\"300\" height=\"200\">\n",
    "        </div>   \n",
    "    </body>\n",
    "</html>\n",
    "'''\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "p_tag = soup.find('p', attrs={'class':'b'})\n",
    "print(p_tag)\n",
    "print('바로 다음 형제태그')\n",
    "print(p_tag.find_next_sibling())\n",
    "print('모든 다음 형제태그')\n",
    "print(p_tag.find_next_siblings())\n",
    "print('바로 이전 형제태그')\n",
    "print(p_tag.find_previous_sibling())\n",
    "print('모든 이전 형제태그')\n",
    "print(p_tag.find_previous_siblings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da91e8d-edaa-4733-88cd-90e78efcba3c",
   "metadata": {},
   "source": [
    "## 검색:find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f91d2f-c060-4436-ab73-63051bf2f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('https://www.naver.com')\n",
    "soup = BeautifulSoup(response.text,'lxml')\n",
    "print(soup.find('title'))\n",
    "print(soup.find('a'))\n",
    "print(soup.find(id='search')) #id속성의 값이 search인 정보를 가져옴 soup.find(attrs={'id'='search'})와 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf90ba97-f626-490c-b3a4-500716cc9f6e",
   "metadata": {},
   "source": [
    "## 검색:find_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1588fe-8a16-4488-a20e-a0401ff20a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = soup.find_all('a', limit=2)\n",
    "print(len(a_tags))\n",
    "print(a_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26acc87-5f41-410c-8b4a-9746ff3cb961",
   "metadata": {},
   "outputs": [],
   "source": [
    "[실습] 네이버 뉴스 페이지에서 언론사 목록 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b636db8-e954-4d42-9fae-2a15a35221f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이낸셜뉴스04월 01일 17:25\n",
      "['파이낸셜뉴스', <span class=\"datetime\">04월 01일 17:25</span>]\n",
      "['파이낸셜뉴스', '신동아', '시사IN', '오마이뉴스', '디지털타임스', '경기일보', '코리아중앙데일리', 'SBS Biz', '미디어오늘', '동아사이언스']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('https://news.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "result = soup.find_all('h4', attrs={'class':'channel'})\n",
    "#print(result[0].get_text())\n",
    "#print(list(result[0].children))\n",
    "press_list = [ list(tag.children)[0] for tag in result]\n",
    "print(press_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3975dd06-0130-4928-b196-2061afae1159",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://news.naver.com')\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "result = soup.find_all('div', attrs={'class':'cjs_age_name'})\n",
    "press_list = [tag.text for tag in result]\n",
    "print(press_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7feb2c4-1107-4349-b134-c6a0f4e3ca1f",
   "metadata": {},
   "source": [
    "## 검색: select_one(), select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39256d9e-cca2-4030-b636-20d2aa6bb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get('http://www.tradecampus.com')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "print(soup.select_one('div > a'))\n",
    "result = soup.select('div a')\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5c1a5-0312-4a78-9cde-75b30597963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select_one('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > div > h3')) # 개발자모드 selector copy하면 나오는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2bfa0-472b-4517-9a03-c4e9bb3dba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice = soup.select('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > ul > li:nth-child(2) > p > a')\n",
    "notice = soup.select('body > div > div.wrapper.main_page > div.renew_main > div.col-12 > div > div.renew_main_notice > div > ul > li > p > a') -> 다 가져오고 싶을때 목록 li 만\n",
    "print(notice[0].txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02274c94-eca8-4488-b7f8-2d285db24f89",
   "metadata": {},
   "source": [
    "## 텍스트 가져오기: text, get_text()\n",
    "-  검색결과에서 태그를 제외한 텍스트만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f02d8-2c63-4cb3-930d-1966a45f6809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고객센터 영역 텍스트 가져오기\n",
    "tag = soup.find('div', attrs = {'class': 'serviceInfo'})\n",
    "print(tag.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914e995-6761-4439-9dde-f32a21776c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.find('img', attrs={'class':'mobile_icon black'})\n",
    "print(tag['src'])\n",
    "print(tag.get('src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7a048-efb1-4d95-b781-458e75fae6fa",
   "metadata": {},
   "source": [
    "## 텍스트 가져오기: string\n",
    "- 검색결과에서 **태그 안에 또 다른 태그가 없는 경우 **해당 내용을 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25261bf9-9367-4938-9045-56e3a5125dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.find('div', attrs = {'class': 'serviceInfo'})\n",
    "tag = tag.find('span')\n",
    "print(tag.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f4845e-9644-4524-9843-63ab1d2b4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[실습] 네이버 웹툰 제목 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359cd90-8b56-4ce7-aeec-1a51a4b97640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4dfae6-8236-4a2e-bc2b-a5b3dcfda8d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92ddef0-96a4-43f8-80c3-1d0f68357546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()),options=ChromeOptions())\n",
    "\n",
    "URL = 'https://comic.naver.com/webtoon'\n",
    "driver.get(URL)\n",
    "time.sleep(4) # 동적으로 생성되는 페이지의 내용이 완성될 때까지 대기\n",
    "soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "\n",
    "# 요일별 전체 웹툰 CSS 선택자\n",
    "temp = soup.select_one('#container > div.component_wrap.type2 > div.WeekdayMainView__daily_all_wrap--UvRFc')\n",
    "\n",
    "\n",
    "# 요일별 div 태그 검색\n",
    "temp = temp.find_all('ul', attrs = {'class':'WeekdayMainView__daily_list--R52q0'})\n",
    "week = ['월','화','수','목','금','토','일']\n",
    "for i, w in enumerate(temp):\n",
    "    print(f'==== {week[i]} 요 웹튼 ====')\n",
    "    webtoon_list = w.find_all('li', attrs = {'class':'DailyListItem__item--LP6_T'})\n",
    "    for webtoon in webtoon_list:\n",
    "        print(webtoon.find('span', attrs={'class':'text'}).text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1ac92-fa3f-4b0e-b3f6-9fddecfbdbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[실습] 메가박스 영화정보 사이트에서 영화 포스터 다운로드 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1174d9d-7cf7-478e-ab2e-7ddccff79972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# 사전 테스트 : 박스 오피스 1위 영화 포스터 이미지 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import ChromiumOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service=Service(ChromeDriverManager().install()),options=ChromeOptions())\n",
    "URL = 'https://megabox.co.kr/movie'\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "poster_img = soup.select('#movieList > li:nth-child(1) > div.movie-list-info > img')\n",
    "print(len((poster_img)))\n",
    "poster_img_src = poster_img[0].get('src')\n",
    "\n",
    "import requests\n",
    "res = requests.get(poster_img_src)\n",
    "with open('poster.jpg','wb') as f:\n",
    "    f.write(res.content)\n",
    "print('End')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42445a04-f83d-43f9-a29e-af20c7c9e895",
   "metadata": {},
   "source": [
    "[문제] 메가박스 영화 사이트에 첫 페이지에 있는 모든 영화 포스트 이미지 수집하기\n",
    "- 메가박스 영화 사이트 첫 페이지에 있는 20개의 영화 포스트 이미지 수집\n",
    "- 현재 작업 디렉토리 밑에 'poster_img' 폴더가 없는 경우 폴더를 생성한다. (os 패키지 이용)\n",
    "- 저장되는 각 포스터 이미지의 파일이름은 영화제목으로 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4979d-4a32-4317-b839-942e7a59fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. '쿵푸팬더4'의 포스터 이미지를 저장했습니다.\n",
      "2. '비키퍼'의 포스터 이미지를 저장했습니다.\n",
      "3. '댓글부대'의 포스터 이미지를 저장했습니다.\n",
      "4. '마더스'의 포스터 이미지를 저장했습니다.\n",
      "5. '파묘'의 포스터 이미지를 저장했습니다.\n",
      "6. '오멘_ 저주의 시작'의 포스터 이미지를 저장했습니다.\n",
      "7. '바람의 세월'의 포스터 이미지를 저장했습니다.\n",
      "8. '기동전사 건담 SEED FREEDOM'의 포스터 이미지를 저장했습니다.\n",
      "9. '씬'의 포스터 이미지를 저장했습니다.\n",
      "10. '고질라 X 콩_ 뉴 엠파이어'의 포스터 이미지를 저장했습니다.\n",
      "11. '패왕별희 디 오리지널'의 포스터 이미지를 저장했습니다.\n",
      "12. '듄_ 파트2'의 포스터 이미지를 저장했습니다.\n",
      "13. '1980'의 포스터 이미지를 저장했습니다.\n",
      "14. '뒤주'의 포스터 이미지를 저장했습니다.\n",
      "15. '극장판 스파이 패밀리 코드 _ 화이트'의 포스터 이미지를 저장했습니다.\n",
      "16. '목스박'의 포스터 이미지를 저장했습니다.\n",
      "17. '영웅본색'의 포스터 이미지를 저장했습니다.\n",
      "18. '남은 인생 10년'의 포스터 이미지를 저장했습니다.\n",
      "19. '로봇 드림'의 포스터 이미지를 저장했습니다.\n",
      "20. '청춘 스케치'의 포스터 이미지를 저장했습니다.\n",
      "모든 포스터 이미지 수집 및 저장이 완료되었습니다.\n"
     ]
    }
   ],
   "source": [
    "#모범답안\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import ChromiumOptions\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "if not os.path.exists('poster_img'):\n",
    "    os.makedirs('poster_img')\n",
    "\n",
    "driver = Chrome(service = Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "\n",
    "\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "movie_posters = soup.select('#movieList > li > div.movie-list-info > img')\n",
    "for i, poster in enumerate(movie_posters, start=1):\n",
    "    movie_title = poster.get('alt').strip().replace(':', '_') # 영화 제목 추출\n",
    "    poster_img_src = poster.get('src') # 영화 포스터 이미지 URL 추출\n",
    "    res = requests.get(poster_img_src) # 영화 포스터 이미지 다운로드 및 저장\n",
    "    with open(f'poster_img/{movie_title}.jpg', 'wb') as f:\n",
    "        f.write(res.content)\n",
    "    print(f\"{i}. '{movie_title}'의 포스터 이미지를 저장했습니다.\")\n",
    "\n",
    "# 작업 완료 후 브라우저 닫기\n",
    "driver.quit()\n",
    "print('모든 포스터 이미지 수집 및 저장이 완료되었습니다.')\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = Chrome(service = Service(ChromeDriverManager().install()), options=ChromeOptions())\n",
    "URL = 'https://www.megabox.co.kr/movie'\n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# 포스터 이미지를 가지고 있는 모든 img 태그를 검색\n",
    "poster_imgs= soup.find_all('img',attrs={'class':'poster lozad'})\n",
    "# print(len(poster_imgs))\n",
    "\n",
    "#이미지를 저장할 폴더 생성\n",
    "import options\n",
    "img_dir = \"./poster_img/\"\n",
    "if not os.path.exists('img_dir'):\n",
    "    os.makedirs('img_dir')\n",
    "    print('폴더 생성 완료')\n",
    "else:\n",
    "    print('폴더가 존재함')\n",
    "\n",
    "for i, poster in enumerate(poster_imgs,1): \n",
    "    title = poster.get('alt')\n",
    "    img_url = poster.get('src')\n",
    "    \n",
    "    print(i,':',img_url)\n",
    "    img_res = requests.get(img_url)\n",
    "\n",
    "    if \":\"in title:\n",
    "        title = title.replace(':',' ')\n",
    "\n",
    "    with open(img_dir+f'[{i}].{title}.jpg','wb') as f:\n",
    "        f.write(img_res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
